# 伸びなかった時代から学んだこと

— データドリブンな意思決定のための共通言語 —

---

## はじめに：なぜこの話をするのか

今日は、僕がこれまでの経験で「痛い目」を見てきたことと、そこから学んだことを共有させてください。

A/Bテストをずっと一人でやってきて、最近良い結果が出てきました。でも、ふと気づいたんです。「そもそもなぜA/Bテストをするのか」がチームに伝わっていないな、と。

スクラム開発の検査フェーズで「この施策って本当に効果あったの？」と振り返ることが、次のフェーズに活きてくる。その「共通言語」を今日つくれたらと思っています。

---

## 1. 伸びなかった時代

### 最初のスタートアップ

最初に入った会社は、ソフトバンクで最年少部長になった方が立ち上げたスタートアップでした。

iPhone普及の立役者で、土日も普通に稼働。夜3時まで仕事して6時に起きる。ほぼ寝ないときは30分しか寝ない。そんな人でした。開発陣も優秀な方が多かった。

**でも、グロースしなかった。**

### 副業での大手企業アプリ開発での経験

その後、トヨタ・バンダイなどの大手企業のアプリ開発をやらせてもらいました。

**それでもアプリの評価は星2程度。**

優秀な人がいる。行動量もある。開発力もある。

> 「じゃあ、なんで伸びないんだろう？」

この問いが、ずっと頭の中にありました。

---

## 2. 転機：データドリブンの業界に入って

前職でハイパーカジュアルゲームというゲーム開発に出会った。

### そもそもハイパーカジュアルゲームって何？

アプリエンジニアの皆さんには馴染みがないかもしれないので、簡単に説明させてください。

**一言で言うと：** 「チャリ走」「Flappy Bird」「2048」のような、誰でもすぐ遊べる超シンプルなゲームのことです。

**特徴：**
- **操作がめちゃくちゃシンプル** — タップだけ、スワイプだけで遊べる
- **チュートリアル不要** — 見た瞬間に遊び方が分かる
- **短いセッション** — 1プレイ数十秒〜数分。電車の1駅分で遊べる
- **広告で稼ぐモデル** — 課金ではなく、広告視聴が収益源

**なぜこの業界がデータドリブンなのか：**

普通のアプリと違って、ハイパーカジュアルゲームは「広告費 vs 広告収益」の戦いです。

1インストールあたりの広告費（CPI）が60円だとして、そのユーザーが広告を見て生み出す収益（LTV）が100円なら黒字。この数字のバランスが全て。だから、数字を見て改善する文化が根付いているんです。

皆さんがYouTubeやSNSで「なんでこんなゲームの広告ばっかり流れてくるんだ？」と思ったことありませんか？

このゲームがなぜ広告として頻繁に流れてくるか知っていますか？

> データドリブンに運営している業界だからです。

### 学びの源泉：日本No.1のナレッジ

ここで得た学びは、日本で4年連続アプリダウンロード数1位の会社（KAYAC）のメイン事業部のナレッジです。

- Kaggleで1ヶ月で金賞クラスを取るような優秀なメンバーと協働
- その事業部を立ち上げた部長から、4回ほど会食で直接学びを得た
- さらに、その部長を引き継いだ方を同じチームに引っ張り、ゲームプランナーとして一緒に仕事した
- セガで長年働いてKAYACに入った平山さんのメソッドも参考にした

> 日本トップクラスの実績を出している人たちから、直接ナレッジを吸収できる環境だった。

### どうやって学んできたか

自分が参考にしてきたのは、本や記事だけではありません。

- 実際に会った人たちから直接聞く
- 海外のデータサイエンティストにLinkedInで質問してみる
- 海外の記事を読み漁る
- いろんなアプリを実際に触ってみる
- 1億ダウンロード達成した人と一緒に仕事してみる

結局、「実際にやっている人」から学ぶのが一番早かった。

### 驚いたこと

VoodooやSupersonic（BeRealを買収した企業）の内部SDKを調査したところ、博士課程を終えたデータサイエンティストがいる環境でも、基本的なやり方はほぼ同じでした。

> 専属のデータサイエンティストがいなくても、基本的なA/Bテストの手法だけで十分対応可能だったんです。

---

## 3. A/Bテストの具体的なやり方

まず、実際にどうやってA/Bテストを回しているかを共有します。

### 基盤構成：Firebase × BigQuery

FirebaseのログをBigQueryに流し込む構成を取っています。

エンジニアとして理解しておくべきポイントは2つ：

1. **FirebaseとBigQueryの連携** — Firebaseで取得したログがBigQueryに自動で流れる
2. **ログにパラメータを足し込める** — これを使ってA/Bの振り分け情報を仕込む

### ユーザーの振り分け方法

A/Bテストのキモは「ユーザーをどう振り分けるか」です。

**やり方：**

1. ユーザーごとに0〜1の乱数を生成する
2. その乱数を元に「A群」「B群」を決める（例：0.5未満ならA、0.5以上ならB）
3. その値をFirebaseの「ユーザープロパティ」に保存する

**ポイント：** ユーザープロパティはBigQueryで全イベントに紐づきます。

つまり、「このユーザーはA群」「このユーザーはB群」という情報が、そのユーザーの全ての行動ログに自動で付与される。

> だから、BigQueryでA群とB群を分けて「イベントに差があるか」を判定できる。

### 段階公開との併用を推奨

コード内での乱数振り分けだけだと、バグ混入時に検出しにくい。

そこで、アプリストアの段階公開（新旧バージョンを50%/50%で配信）も併用します。

段階公開で「数字が下がっている」→「バグが混入しているかも」が分かりやすくなります。

### 統計的な判断基準

「A群：コンバージョン率10%」「B群：12%」だった場合、すぐに「B採用！」と言えるでしょうか？

この差が本当にBの施策が良いから生まれたのか、たまたま偶然生まれた差（誤差）なのかを区別する必要があります。

**サイコロの例：** 10回振って6が3回出ても「6が出やすい」とは言えない。でも1000回振って600回出たら「おかしい」と言える。

t検定は「この差がたまたまじゃない確率」を計算する方法です。P値 < 0.05 なら「統計的に有意」と判断するのが一般的です。

**P値の解釈**

| P値 | 解釈 |
|-----|------|
| P = 0.5 | 50%の確率で偶然。全然信用できない |
| P = 0.1 | 10%の確率で偶然。まだ怪しい |
| **P = 0.05** | **5%の確率で偶然。一般的にここが基準** |
| P = 0.01 | 1%の確率で偶然。かなり信頼できる |

### 結果別の対応

| 結果 | 対応 |
|------|------|
| **勝ち** | 採用する |
| **負け** | 採用しない。磨き込むか損切りかを判断 |
| **中立** | 開発が円滑に進む方を採用 |

**コア機能 vs 非コア機能**

- **コア機能**（これがないとアプリが成立しない）→ 勝つまで磨き込む選択肢あり
- **非コア機能** → ズルズル防止のため、早めに判断してやめてもOK

投資家的な視点で「この分野に投資した方がリターンが大きいか」を考え、時には損切りも大事です。

---

## 4. A/Bテストをやって気づいたこと

実際にA/Bテストを回してみて、いくつかの大きな気づきがありました。

### 気づき①：A/Bテストは「絶対に悪くならない」ツール

A/Bテストって何かというと、シンプルに言えば「絶対に良い方を採用する」ツールです。

AとBを比較して、良い方だけを採用する。それを繰り返す。

> つまり、絶対に悪くはならない。絶対に良い方向にしか進まない。

機能開発して全部良くなるなら、すべての会社が成功しているはずです。そうではないからこそ、「良いものだけを採用していく」仕組みが必要でした。

A/Bテストがしっかりしている企業は伸びているという統計結果があります。良い方を採用し続ければ、伸びるしかない。単純だけど、やっていなかった。

### 気づき②：自分たちでコントロールできる変数にフォーカスできる

事業をやっていると、日々の数字の推移に一喜一憂しがちです。

でも、その推移の中には「自分たちではどうにもできない外的要因」も含まれています。市場全体のトレンド、競合の動き、季節要因...。

A/Bテストの良いところは、「自分たちのアクションに対してどうなったか」だけを見られること。

> マーケットに左右されない、自分たちで動かせる変数にフォーカスできる。

だから、アクションを取りやすい。自分たちの打ち手に対するフィードバックがもらえる。「じゃあ次はどう変えればいいか」が分かる。

漠然と「数字が下がった...」と落ち込むのではなく、「この施策は効いた/効かなかった、だから次はこうする」という形で、伸ばせることにフォーカスできるようになります。

### 気づき③：勘には限界がある（でも磨ける）

スティーブ・ジョブズは別格ですが、勘は自分の得意分野でしか再現性がありません。

積み上げてきた結果として勘が生まれるのであって、知らない分野で急に勘でやっても無理なんです。

> 再現性を持つためには、データを見て「どれが良かったか・良くなかったか」をA/Bで検証していく。その中で勘が身についていく。

多分ジョブズも、内々でいろんなフィードバックをもらって「これだといける」「いけない」を小さく積み重ねる中で、感覚を磨いていったんだと思います。天才に見える人も、検証の積み重ねがある。

### 気づき④：10億DL達成者の頭の中

10億ダウンロードを達成した方と話して驚いたのは、頭の中でA/Bテストの結果に基づいたシミュレーションが常に回っている状態だったことです。

マーケット選びから各A/Bテストの結果まで、シミュレーションができている。データで磨かれた勘は、最終的に精度の高い直感になる。得意分野なら8割方ヒットを出せるレベルです。

ただし新しい分野（例：パズルゲーム）ではまだ知見がないので当てられないこともある、とも言っていました。勘は「積み上げた結果」なんです。

### 気づき⑤：「どこに手を入れるか」が全て

A/Bテストをやっていて、一番大きな学びはこれかもしれません。

> 結局、ユーザーに便益を与えないと意味がない。

A/Bテストを繰り返していくと、「当て感」が分かってきます。

本当に工数が少ない施策、例えば画面の順番を変えたり、コア機能のほんの少しの体験変更だけで、めちゃくちゃ伸びることがある。

逆に、何週間もかけて作った周辺機能が全然数字に影響しないこともある。

**A/Bテストをやる前は分からなかった：**
- 周辺機能（オプション設定、追加機能など）を一生懸命作っていた
- でもコア体験がユーザーにフィットしていなければ、周辺をいくら磨いても意味がなかった

**だから大事なのは：**
1. **まずコア体験を磨く** — ユーザーが「これがないと使わない」という部分
2. **ユーザーの使用シーンを理解する** — 電車の中？集中できる環境？
3. **小さく試して、当たりを見つけてから大きく投資する**

> 「どこに手を入れるか」で、少ない工数でも結果は全然違う。これはA/Bテストを繰り返さないと分からなかった。

---

## 5. よくある勘違いを解いておく

### 勘違い①：「小さいテストしかできない」

「A/Bテストって、ボタンの色を変えるとか、そういう小さいテストだけでしょ？」

**全然そんなことないです。**

大きな機能追加も、フロー全体の変更も、A/Bテストの対象になります。むしろ大きな変更こそ、A/Bテストで検証すべきです。開発コストが高い施策ほど、「本当に効果があるのか」を確認してから全ユーザーに展開した方がリスクが低いからです。

### 勘違い②：「複数テストは同時に回せない」

影響範囲が被らなければ、複数テストの同時並行は可能です。開発スピードを落とさないためにも、積極的にやるべきです。

### 勘違い③：「過去の勝ちパターンを繰り返せばいい」

A/Bテストでうまくいったパターンを参考にするのは大事です。でも、それを「擦る」だけでは限界があります。

大事なのは、いろんなインプットをすること。他社の事例、ユーザーの声、自分で考えたアイデア...。そこから「これをやったらもっと良くなるんじゃないか」という確度の高い仮説を持ってくる。

その仮説を磨き上げて、機能として実装して、A/Bテストで検証する。

> 過去の勝ちパターンは「守り」、新しい仮説を試すのは「攻め」。両方あってこそ伸びていける。

---

## 6. 応用：コード以外にも使える

A/Bテストはコード上だけの話ではありません。

「昨日のツイートと今日のツイートどっちが伸びた？」という比較も、条件がある程度揃っていて、一定のn数があり、10〜20%以上の改善が見られたら判断しやすいです。

完璧を求めすぎず、大きな差があるかどうかをまず見る。迷ったら検定する、くらいでOKです。

### Webサービス・ビジネスアプリへの応用

ゲーム（非言語分野）は変数が多く真似しにくいですが、Webサービス・ビジネスアプリは言語化されている部分が多く、応用しやすいです。

Netflix、Amazonなど大手テック企業のA/Bテスト事例や、行動経済学の研究など、すでに効果が証明されているものは積極的に取り入れやすいです。

---

## 7. 今後どう活かすか

スクラム開発の検査フェーズで、こんな振り返りができるようになれたらと思っています。

> 「この施策、数字で見るとどうだった？」

> 「勝ち/負け/中立、どれだった？」

> 「次のスプリントで検証すべきことは何だろう？」

この「共通言語」があれば、感覚ではなくデータに基づいた議論ができるようになります。

## 8. 実際にどういう感じで回してたか

ここからは、実際にどんな感じでA/Bテストを回していたかを共有します。

### 開発サイクル：週1〜2回のリリース

v0.2 → v0.3 → ... → v1.0.5 と、約3ヶ月で20回以上のバージョンアップを行いました。

**1週間の流れ：**
1. 火曜：新バージョンをリリース、ABテスト開始
2. 水〜木：データ収集（最低でも数千ユーザー）
3. 金曜：結果を見て「採用/不採用/継続」を判定
4. 土〜月：次の改善を実装
5. 火曜：次バージョンをリリース...

> ベストは火曜と金曜のリリース。火曜に出さないと金曜に意思決定が全くできない。

### ABテストの実例：同時に複数テストを回す

ある週のテスト内容はこんな感じでした：

| テスト | 内容 |
|--------|------|
| AB1 | スローモーション：従来 vs 敵に当たってから発動 |
| AB2 | 操作性：従来 vs 回転しやすく調整 |
| AB3 | 透明壁：あり vs なし |
| AB4 | キャラのHP：なし vs あり |
| AB5 | 表示：レベル番号 vs 撃破数カウンター |
| AB6 | 画面切り替え：通常 vs 高速化 |

影響範囲が被らなければ、同時に開発スピードまにあえば6個くらい回せます。

### ダッシュボードで見ていた指標

BigQueryで集計して、こんな感じのダッシュボードを作っていました：

| 指標 | 説明 |
|------|------|
| D1リテンション | 翌日も起動した割合（目標：30%以上） |
| インステ視聴回数 | 広告視聴回数（収益に直結） |
| アドスコア | 広告視聴の総合スコア |
| 平均プレイ時間 | 1セッションの長さ |
| MaxStage平均 | どこまで進んだか |
| Stage10離脱率 | 序盤の離脱（初期体験の良さ） |

### 判定の実例

実際のデータと判定はこんな感じでした：

**例1：スローモーション（AB4）**

| パターン | ユーザー数 | D1リテンション | インステ視聴 |
|----------|-----------|---------------|-------------|
| 0：従来のスロー | 1,764 | 22.15% | 12.706 |
| 1：敵に当たってから発動 | 1,915 | 20.10% | 10.220 |

→ **0採用**（従来の方がスコアが良い）

**例2：画面切り替え速度（AB6）**

| パターン | ユーザー数 | インステ視聴 | プレイ時間 |
|----------|-----------|-------------|-----------|
| 0：通常 | 2,732 | 10.76 | 9分18秒 |
| 1：高速化 | 2,150 | 10.76 | 9分01秒 |

→ **1採用**（スコア同等で、UXが良い方を採用）

**例3：決着がつかない場合**

| パターン | D1リテンション | インステ視聴 |
|----------|---------------|-------------|
| 0 | 34.41% | 10.18 |
| 1 | 34.83% | 10.31 |

→ **継続**（差が小さいのでもう少しデータを集める）

### 3ヶ月間の改善の積み重ね

| 週 | CPI | D1リテンション | 広告視聴回数 |
|----|-----|---------------|-------------|
| 0週目 | $0.40 | 15% | 3.0 |
| 1週目 | $0.38 | 17.8% | 4.3 |
| 2週目 | $0.38 | 21.1% | 5.4 |
| 3週目 | $0.36 | 20.1% | 7.6 |
| 4週目 | $0.36 | 25.6% | 7.0 |
| 5週目 | $0.34 | 28.5% | 8.9 |

**D1リテンションが15%→28.5%に改善。** 約2倍です。

これは「一発で当てた」のではなく、毎週の小さな改善の積み重ねでした。

### 効いた施策・効かなかった施策

**効いた施策：**
- ステージ追加（数十ステージ単位）
- ボイスSE追加
- 画面切り替え秒数の短縮
- ステージの印象に個性を持たせる

**効かなかった施策：**
- キャラをマネキンに変更（むしろ悪化）
- ホールド中スロー機能

**どちらでもなかった施策：**
- フライテキスト（HEADSHOT!など）
- COMPLETE表示の有無
- 弾痕エフェクト

> 「絶対良くなる」と思った施策が効かないこともある。だからこそテストする。

### チームでの学習サイクル

週次で振り返りを行い、ナレッジを蓄積していました。

**毎週やっていたこと：**
- 今週やったこと / 翌週やることを整理
- 効いた施策・効かなかった施策を記録
- 「なぜ効いたか」「なぜ効かなかったか」を言語化

**大事だったこと：**
- 「効かなかった」も価値あるナレッジとして扱う
- 同じ失敗を繰り返さない
- 次のプロジェクトに活かせる形で残す

> プランナーと「この数字どう思う？」を気軽に話せる関係が理想。資料ベースではなく、ダッシュボードを一緒に見ながら壁打ちする。

---

## まとめ

1. 優秀な人と行動量だけでは伸びない。僕はそれを痛感した
2. A/Bテストで「良いものだけを採用する」仕組みが必要
3. 勘は積み上げた結果。データで磨かれた勘は、最終的に精度の高い直感になる
4. 難しい統計は不要。基本的な手法で十分対応可能
5. 検査フェーズでの振り返りを「共通言語」で行えるようになろう

---

## ＋α：マーケット選びもデータドリブンに

ここまではA/Bテストの話でしたが、実はその前段階、「そもそもこのマーケットでやるべきか」もデータドリブンに判断できます。

### CPIテストという手法

ハイパーカジュアルゲームでは「CPIテスト」というものをやっていました。

本当にMVPレベルのものを作って、実際に広告を流してみる。そこで「1インストールあたり何ドルかかるか（CPI）」を見る。僕らの場合は0.4ドル=60円以下を基準にしていました。
MVPレベル = ステージは3ステージとか

**具体的な検証コスト：** 1回のテストで広告費は4万円だけ。それでいけるかどうかを判定します。

もし筋が良さそうだったり、「広告のクリエイティブをこう変えたらもっといけそう」と思ったら、もう一度4万円だけ流してみる。このサイクルを回していく。

> プロダクトを作り込む前に、マーケットにフィットするかを小さく検証する。

### 最初は時間がかかる、でも...

最初の頃は、CPIテストを通るまで1年くらいかかりました。

でも繰り返していくうちに、「これはいける」「いけない」が分かってくる。

最終的には2〜3週間で1本当てられるサイクルになりました。2〜3日で小さいMVPを作って、週に2本くらいCPIテストを回すペースです。

### 判断基準を持つ

成功している会社は、社内基準を設けています。

例えば「ダダサバイバー」を作っている会社は、「1日目継続率（Day1 Retention）が60%以上」という基準を設けていたりします。

僕らも最初は0.4ドルを厳格に守っていましたが、経験を積むと「CPIは基準を超えてないけど、リテンションや起動回数、エンゲージメント時間を見るといけそう」という判断もできるようになります。

### 業界別リテンションの参考値

マーケットによって特性は変わります。例えばフィットネスアプリはリテンションが長い傾向にあります。

各業界のリテンション目安は、DeepResearchなどで調べると出てきます。それを参考に「このマーケットならこれくらいの数値を目指す」という自分なりの軸を持っておくと、マーケットフィットの判断がしやすくなります。

### もっと軽くやる方法もある

実はプロダクトを作らなくても検証できることもあります。

簡単なLPを作って広告を流し、クリック率を見るだけでも「このコンセプトに興味を持つ人がどれくらいいるか」は分かる。0.4ドルでクリックまで至っているなら、ある程度の需要はありそう、という判断ができます。

> 自分なりの判断軸を持って、小さくマーケットに打ってみる。これもA/Bテストと同じで、繰り返すうちに直感として機能するようになります。

---

**ご清聴ありがとうございました**