---
created: 
tags: 
related:
---

【🔵主なライブラリ】
* NumPy - 数値計算を効率的に行うための拡張モジュール。
* Pandas - データ解析を支援する機能を提供するライブラリ。
* matplotlib - グラフ描画のライブラリ。Pandasオブジェクトのグラフ描画メソッドで利用している。
* Seaborn - matplotlib のラッパー。matplotlib をそのまま使うと面倒なグラフが簡単に描ける。
* scikit-learn - オープンソース機械学習ライブラリ。

【🔵Kaggleの用語】
* Competition
    * コンペ。コンペに参加する場合、コンペに紐づくNotebookを作成します。 補足） 分類／回帰予測のモデルを作成し、その精度を競います。 コンペは期間限定で開催されます。 コンペのページに概要、評価の方法、賞金について、などの記載があります。 今回参加するタイタニック生存者予測はチュートリアルの位置づけです。
* Datase
    * 色々なデータが公開されています。自分でデータをuploadしてDatasetを作成することができます。
* Notebook
    * ソースファイルみたいなもの。昔(2019/07以前)はKernelと呼ばれていました。
* New Notebook
    * 新規に作成する場合JupyterNotebook形式の’Notebook'か、1ファイル形式の'Script'かを選びます。
* Leaderboard
    * スコアのランキング。参加者が特定のデータに過学習するのを防ぐため、非公開スコアと公開スコアがあります。モデルが過学習している場合、モデルを訓練したデータ以外のデータには適合しません。これは、類似の別のサンプルでは、モデルの精度が低くなることを意味します。
* Public Leaderboard
    * テストデータの予測 (サブミットしたデータ)の50%が公開スコアに割り当てられます。
* Private Leaderboard
    * 残り50%の予測は非公開スコアに割り当てられます。参加者は非公開スコアをコンペが終わるまで、見ることができません。非公開スコアは、コンペの勝者を決定するために使用されます。なお、Getting started competitionsでは、非公開スコアは公開されません。

【🔵KaggleのNotebookの操作について】
* Shift+Enterでセルの内容を実行
* print文を書かなくとも、セル内の最終行の結果が出力される
* 現在セルの上まで実行（Run Before）、全て実行（Run All） がある
* 上のセルに戻って、コード追加/セル追加して実行できるけど、後で順番に実行する際に予期しないエラーになるかも

【🔵使ったメソッド】
* train_df.groupby(["Sex", "Pclass", "Survived"])
    * データフレームtrain_dfをSex（性別）、Pclass（チケットクラス）、Survived（生存状況）の各カテゴリに基づいてグループ分けしています。例えば、女性で1等クラスのチケットを持ち、生存した乗客のグループなどです。
* .size()
    * 各グループのサイズ（つまり、乗客の数）を計算します。
* reset_index()
    * グループ化によって作成されたインデックスをリセットし、通常の列（カラム）に変換します。これにより、Sex、Pclass、Survivedが通常の列としてデータフレームに含まれます。
* rename(columns=({0: "count"}))
    * グループごとの乗客数を示す列（もともとの列名は0）をcountという名前に変更します。
* train_df["Age"].median()
    * 中央値を出すメソッド
* df.dtypes
    * データの型を見るメソッド

🔵目的変数、説明変数 目的変数：予測したい値 説明変数：目的変数を説明している値

* Kaggle Rankings
    * Kaggle におけるランキング表を確認できます。
*  Competitions
    * Kaggle で開催されているコンペティションを確認できます。
*  Datasets
    * Kaggle 上で共有されているデータセットを確認できます。
*  Code (Notebook・Kernel)
    * クラウド上でコードを実行できる環境が使用できます。
*  Discussion
    * コンペティションの内容に関わる議論ができる掲示板を確認できます。

🔵データの前処理
* ワンホットエンコーディング
    * カテゴリ毎に新しい列を作成し、該当するカテゴリの列にのみ1を、それ以外には0を割り当てる手法
    * この方法では、各カテゴリが等しく扱われ、順序関係を示さないため、名義尺度のカテゴリカルデータに適す
    * <例>
        * from sklearn.preprocessing import OneHotEncoder
        * import pandas as pd
        * # インスタンスの作成
        * onehot_encoder = OneHotEncoder(sparse=False)
        * # カテゴリデータ（2次元配列が必要）
        * categories = [['Red'], ['Blue'], ['Green']]
        * # ワンホットエンコーディングの適用
        * onehot_encoded = onehot_encoder.fit_transform(categories)
        * # DataFrameに変換して見やすくする
        * df_onehot = pd.DataFrame(onehot_encoded, columns=onehot_encoder.get_feature_names_out())
    * ワンホットエンコーディングは新しい列を多く作るため、特徴量の次元が大きくなる（次元の呪い）。
    * このため、特徴量選択や次元削減の技術も併用することがある。
        * ラベルエンコーディングと異なり、ワンホットエンコーディングでは元のカテゴリの数だけ特徴量の次元が増加する。
        * そのため、カテゴリの数が多い場合にはデータセットが非常に大きくなる可能性があり、その場合は次元削減の技術や、より高度なカテゴリエンコーディング手法（例えば、平均エンコーディングやエンティティ埋め込み）を検討する必要がある 
* ラベルエンコーディング
    * 各カテゴリに一意の整数を割り当てる手法
    * <例>[“Red", "Blue", "Green"] というカテゴリがある場合
        * これらにそれぞれ 0, 1, 2 というラベルを割り当てる
    * ⚠ラベルエンコーディングはカテゴリ間に順序関係がある場合に適していますが、順序関係がない（例えば色や都市の名前など）場合には、モデルが不適切な関係を学習してしまう可能性がある
    * <例>
        * from sklearn.preprocessing import LabelEncoder
        * # インスタンスの作成
        * label_encoder = LabelEncoder()
        * # カテゴリデータ
        * categories = ['Red', 'Blue', 'Green']
        * # ラベルエンコーディングの適用
        * labels = label_encoder.fit_transform(categories)
* 欠損値の処理
    * 欠損値の処理方法として、平均値（mean）や中央値（median）を使用するかは、データの分布と欠損の性質による。
    * データの特性を詳しく調べることで、平均値や中央値のどちらを使うべきか、あるいは他の方法を採用するべきかを決定することができる。
        * <例>欠損値が多い列に対しては、その列を削除するか、あるいは他の変数から予測するなどの方法もある。
        * 欠損値を補完する際には、データセットのサイズや特徴量の重要性、予測モデルの目的なども考慮する必要がある。
    * 最適な方法を選択するためには、以下のポイントを考慮することが重要
    * 平均値を使う場合：
        * データが正規分布している場合、または外れ値がほとんどない場合に適している。
        * 平均値は外れ値によって大きく影響を受けるため、外れ値がある場合は適さない可能性がある。
    * 中央値を使う場合：
        * データが歪んでいる（スキューがある）場合や、外れ値が含まれる場合に適している。
        * 中央値は外れ値の影響を受けにくく、より堅牢な指標。
    * その他の考慮点：
        * 欠損値がランダムに発生しているかどうか（Missing Completely at Random, MCAR）
        * あるいはデータの他の値に依存して発生しているかどうか（Missing at Random, MAR）
        * あるいは欠損そのものが何らかの情報を持っているかどうか（Missing Not at Random, MNAR）
        * によって、処理の方法を変える必要があります。
    * ⚠一部の機械学習アルゴリズムは、欠損値を扱うことができるため、場合によっては欠損値をそのままにしておくことも選択肢になります。
【他の指標による補完】
    * 最頻値（Mode）:
        * 特にカテゴリカルデータに適しており、最も頻繁に出現する値で欠損値を補完する。
    * 時系列データの場合の前後のデータによる補完:
        * 前のデータ（前方補完）や次のデータ（後方補完）で補完する方法。
        * 時系列の連続性が重要な場合に有効。
    * 多変量代入法（Multiple Imputation）:
        * 欠損値を含む変数と他の変数間の関係をモデル化して、欠損値を推定。これには、回帰代入やk-NN代入などがあります。
    * k-最近傍法（k-NN Imputation）:
        * 欠損値を持つサンプルに最も類似しているk個のサンプルの値を参考にして欠損値を補完する。
【高度な手法】
    * ランダムフォレスト:
        * 欠損値を予測するためにランダムフォレストを使用する手法もある。
    * ディープラーニング:
        * ニューラルネットワークを使って欠損データを予測する手法も研究されている。
【専門的なアプローチ】
    * ドメイン知識:
        * データの背景にある専門知識を利用して、適切な値を推定する。
        * <例>医療データでは、ある検査値の欠損は特定の病状を示唆することがある。
    * データのパターン:
        * データ内の他のパターンや傾向から欠損値を推測することも可能。
【欠損値をそのまま使う】
    * 欠損値インジケータ:
        * 欠損値が何らかの情報を含んでいると考えられる場合は、欠損を示すインジケータ変数を作成し、それをモデルに含めることもある。
🅿️どの方法が最適かは、
        - データの種類
        - 欠損値のパターン
        - 分析の目的
    - によって異なります。
    - また、欠損値の補完方法を選択する際には、補完後のデータが元のデータの分布をどの程度保持しているか、モデルのバイアスにどのような影響を与えるかを検討することが重要
    - 実際には、複数の手法を試し、クロスバリデーションや様々な評価指標を用いて、モデルの性能を比較検討することが一般的。

🔵データサイエンス分析の一般的な流れ  1. 問題の定義
* 目的（この場合は住宅価格の予測）を明確に定義します。
* どのような特徴が価格に影響を与える可能性があるかを考慮します。
2. データの収集
* 使用するデータセット（例：住宅の特徴、過去の販売価格など）を収集する。
3. データの前処理
* データのクリーニング：欠損値の処理、外れ値の検出と処理。
* 特徴量エンジニアリング：新しい特徴量の作成、不要な特徴量の削除。
* データ変換：カテゴリカルデータのエンコーディング、数値データのスケーリング。
4. 探索的データ分析（EDA）
* データの視覚化と統計分析を通じて、データの特徴と関係性を理解します。
* 相関分析を行い、目的変数と他の特徴量との関係を確認します。
5. モデルの選択と訓練
* 複数の機械学習モデル（例：線形回帰、決定木、ランダムフォレスト、勾配ブースティングなど）を選択します。
* 訓練データを使用してモデルを訓練する。
* クロスバリデーションや他の手法を使用して、モデルの性能を評価します。
6. モデルの評価
* テストデータセットを使用してモデルの性能を評価します。
* 性能指標（例：平均絶対誤差、平均二乗誤差、R2スコアなど）を用いて、予測の精度を評価します。
7. モデルのチューニング
* ハイパーパラメータのチューニングを行い、モデルの性能を最適化します。
* 必要に応じて特徴量選択を行い、モデルの複雑さと性能をバランスさせます。
8. 結果の解釈と展開
* モデルの予測結果を解釈し、ビジネス上の意思決定に役立てます。
* モデルを本番環境にデプロイし、新しいデータに対する予測を行います。

4.🔵探索的データ分析（EDA）
* データセットの特徴と関係性を理解するために非常に重要
* EDAのプロセスでは、データの視覚化と統計分析を行い、データの概要を把握し、特徴量間の関係やデータの分布を調べる
* 以下の手法を用いて、データの特徴と関係性を理解することができる。
    * EDAの結果は、モデル選択や特徴量エンジニアリングの方針を決定するための基礎となる。
* 1. 単変量分析
    * 基本統計量の確認
        * .describe() メソッドを使用して、各数値特徴量の平均値、標準偏差、最小値、最大値などの基本統計量を確認します。
        * data.describe()
        * 分かること
            * 平均値（Mean）
                * 各数値型変数の平均値。
                * これにより、データの中心的な傾向を把握できます。
            * 中央値（Median）
                * 数値型変数の中央値（または第2四分位数）。
                * 平均値と比較して、外れ値の影響を受けにくいデータの中心的な傾向を示します。
            * 最小値（Min）
                * 各変数の最小値を知ることができる。
                * これはデータの範囲の下限を示す。
            * 最大値（Max）
                * 各変数の最大値を知ることができる
                * これはデータの範囲の上限を示す。
            * 標準偏差（Std）
                * 各変数の標準偏差を示し、データがその平均値からどれだけ散らばっているかを示す。
                * 大きな標準偏差はデータが平均値から広範囲にわたって分布していることを意味する。
            * 第1四分位数（25%）
                * データの下位25%の値。
                * これはデータの下部分の範囲を示す。
            * 中央値(50%): 中央値。
            * 第3四分位数（75%）:
                * データの上位25%の値。
                * これはデータの上部分の範囲を示す。
    * ヒストグラム
        * 各数値特徴量の分布を確認します。これにより、データの偏りや外れ値の存在を視覚的に確認できる
        * ￼
* 2. 多変量分析
    * 散布図（Scatter Plot）
        * 2つの数値特徴量の関係を視覚化します。これは特に、目的変数との関係を理解するのに有効。
        * 説明変数が多い場合、sns.scatterplot() を使用してすべての変数のペアに対する散布図を個別に描画するのは現実的ではない。代わりに、以下のような方法を検討できる
            * 1. 散布図行列（Pair Plot）
                * 散布図行列を使用すると、複数の変数間の関係を一度に視覚化できます。Seabornのpairplot 関数を使うことで、データフレーム内のすべての数値変数のペアに対する散布図と各変数のヒストグラムを描画できます。
                * <例># 散布図行列
                * sns.pairplot(train_df)
                * plt.show()
            * 2. 目的変数に焦点を当てた散布図
                * 目的変数との関係に焦点を当て、各説明変数と目的変数との関係を一つずつプロットします。これにより、どの変数が目的変数に大きく影響を与えているかを視覚的に確認できます。
                * <例># 目的変数との関係に焦点を当てた散布図
                * for col in train_df.columns: 
                    * if train_df[col].dtype in ['int64', 'float64']:
                    *  sns.scatterplot(x=col, y='目的変数', data=train_df)
                    * plt.show()
            * 3. 相関行列に基づく選択
                * 変数が多い場合、相関行列を計算して、目的変数と高い相関を持つ変数を選択し、それらの変数についてのみ散布図を描画する。
                * <例># 相関行列
                * corr_matrix = train_df.corr()
                * # 目的変数との相関が高い変数を選択
                * high_corr_variables = corr_matrix['目的変数'].sort_values(ascending=False)
                * # 高い相関を持つ変数に対して散布図を描画
                * for col in high_corr_variables.index:
                    * sns.scatterplot(x=col, y='目的変数', data=train_df)
                    * plt.show()
            * 4. 変数の削減
                * 特徴量選択や次元削減の技術（例：主成分分析（PCA））を使用して、変数の数を減らすことも検討できる。
                * これにより、データセットの扱いやすさと解釈可能性が向上する。
* 相関行列とヒートマップ
    * .corr() メソッドを使用して特徴量間の相関係数を計算し、ヒートマップで表示する。
    * これにより、特徴量間の線形関係の強さを視覚的に把握できる。
* 3. カテゴリカルデータの分析
    * 棒グラフ（Bar Plot）
        * カテゴリカルデータの各カテゴリの出現頻度を表示します。
    * 箱ひげ図（Box Plot）
        * カテゴリごとの数値データの分布を比較。
        * これにより、中央値、四分位範囲、外れ値を確認できます。
* 4. 欠損値の分析
    * 欠損値の有無を確認し、欠損値が存在する特徴量を特定。
    * 欠損値が多い特徴量は、後の分析において特別な注意が必要。
* 5. 目的変数の分析
    * 目的変数（この場合は住宅価格）の分布をヒストグラムや箱ひげ図で確認。
    * 目的変数が正規分布していない場合は、変換（例：対数変換）を検討する。
* 基本統計の確認
* データタイプの確認
    * 各列のデータタイプを確認
    * data.dtypes
    * <例>
        * Id                 int64
        * MSSubClass         int64
        * MSZoning          object
        * LotFrontage      float64
        * LotArea            int64
        *                   ...   
        * MoSold             int64
        * YrSold             int64
        * SaleType          object
        * SaleCondition     object
        * SalePrice          int64
        * Length: 81, dtype: object
* 変数間の関係の探索
    * 数値型変数間の相関係数を確認
    * plt.figure(figsize=(10, 8))
    * sns.heatmap(data.corr(), annot=True, fmt=".2f")
    * plt.show()
* 異常値の確認
    * 異常値の確認には箱ひげ図が有効。
    * <例>家の価格（'SalePrice'列と仮定）について確認。
    * plt.figure(figsize=(8, 6))
    * sns.boxplot(data['SalePrice'])
    * plt.title('Box plot of SalePrice')
    * plt.show()
* カテゴリ変数の分析
    * <例>あるカテゴリ変数（'MSZoning'列と仮定）の頻度を確認します。
    * plt.figure(figsize=(8, 6))
    * sns.countplot(data['MSZoning'])
    * plt.title('Frequency of MSZoning Categories')
    * plt.show()

5. 🔵モデルの選択と訓練
    1. 特徴量選択
        * 目的変数と強い相関関係を持つ特徴量を選択します。
        * 相関が非常に弱い、または無関係と思われる特徴量は除外するか、追加分析を行って関連性を検討する。
    2. データセットの分割
        * モデル訓練のためにデータセットを訓練セットとテストセットに分割する。
        * 通常はデータの70-80%を訓練に、残りの20-30%をテストに使用します。
        * <例>from sklearn.model_selection
        * import train_test_split
        * # 特徴量と目的変数を定義 X = train_df.drop('SalePrice', axis=1)
        * # SalePriceを除いた特徴量 y = train_df['SalePrice']
        * # 目的変数
        * # 訓練セットとテストセットに分割
        * X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    * 3. モデルの選択
        * 複数の機械学習モデルを選択して、それぞれについてハイパーパラメータのデフォルト値を使用して初期評価を行う。
        * <例>from sklearn.linear_modelimport LinearRegression
        * from sklearn.ensemble import RandomForestRegressor
        * from sklearn.metrics import mean_squared_error
        * # モデルのインスタンス化
        * models = {
            * "Linear Regression": LinearRegression(),
            * "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42)
            * # 他のモデルも追加可能
        * }
        * # モデルを訓練し、初期評価を行う
        * for name, model in models.items():
        * model.fit(X_train, y_train)
        * y_pred = model.predict(X_test)
        * mse = mean_squared_error(y_test, y_pred)
        * print(f"{name} MSE: {mse}")
    * 4. モデルの評価
        * クロスバリデーションを使用して、モデルの性能をより厳密に評価する。
        * <例>from sklearn.model_selection import cross_val_score
        * # クロスバリデーションでモデル評価
        * for name, model in models.items(): scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)
        *  rmse_scores = np.sqrt(-scores)
        * print(f"{name} Cross-validated RMSE: {rmse_scores.mean()}")
5.🔵モデルのチューニング【ハイパーパラメータチューニング】
    * グリッドサーチやランダムサーチを使用して、最も性能の良いハイパーパラメータを見つけます。
    * <例>from sklearn.model_selection import GridSearchCV
    * # パラメータグリッドの例
    * param_grid = {
        * 'n_estimators': [100, 200, 300],
        * 'max_features': ['auto', 'sqrt'],
        * 'max_depth': [10, 20, 30]
    * }
    * # グリッドサーチ
    * grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)
    * grid_search.fit(X_train, y_train)
    * # 最適なパラメータを表示
    * print(grid_search.best_params_)

6.🔵 最終モデルの選択
    * 上記の評価結果に基づいて、最も性能が良いと思われるモデルを選択する。

* Kaggleの場合は最後提出
    * X_test = pd.read_csv("/kaggle/input/house-prices-advanced-regression-techniques/test.csv")
    * # テストデータに同じ前処理を適用する
    * test_ids = X_test['Id']
    * # 前処理
    * 〜〜〜〜〜〜
    * # テストデータセットに対する予測
    * predictions = best_model.predict(X_test)
    * # 提出用のDataFrameを作成
    * submission_df = pd.DataFrame({'Id': test_ids, 'SalePrice': predictions})
    * # CSVファイルとして保存
    * submission_df.to_csv('submission.csv', index=False)



