---
created: 2023-11-14
tags: #データサイエンス #統計 #ビジネス分析
related:
---
野村総合研究所データサイエンスラボ 

* メンタルマップ
    * なぜこの本を読もうと思ったか？この本から何を得たいか？読んだ後、どのような状態になりたいと思っているか？
        * 統計の知識とデータサイエンスの全体像をざっくり理解して、専門的な本を読めるようになりたい
        * データサイエンスの知識を知った上でどんなことができるかざっくり理解したい
        * ユニバの人のユニバ伸びを見て、データサイエンスとかAIがそこまで発達していない時でも、統計駆使して、各施策の成功率を極限まで高めることができるため、データサイエンス周りの必須だなと思ったから
        * データ分析してるのに、データサイエンス周りの知見が全くない
        * 単純に知らない領域は面白い

『サイエンティストとは』
* データを単純に収集、整理、分析するだけの人でなく、データをビジネスに活用できる人を指す
* 求められるスキル3つ
    * ビジネス力
        * 分析結果をビジネスに活用すること
            * 課題を理解した上で、ビジネス課題を整理し、解決する力
            * 新たな真実を見つけだし、ビジネスに反映させなければならない
    * データサイエンス力
        * 分析すること
            * 誤報処理、人工知能、統計学などの情報関係の知恵を理解し、使う力
    * データエンジニア力
        * データを処理すること
            * 意味のある形に使えるようにし、実装、運用できるようにする力

『活用事例』
* ダイナミックプライシング
    * 価格を最適化
        * 実績データを用いて、売れ行きを予測し、販売の状況に応じて最も利益が見込める価格を提示する仕組み
        * データサイエンスをもとに、何をどんな時にいくらで売るのかを設定している
            * 掲示価格別の需要、予測が最も重要
                * 過去の実績だけでなく、イベントまでの日数、気象状況、想定される購入者の属性などを考慮して、需要を予測する必要がある
    * <例>2019年シーズンから横浜F・マリノス導入、Uberの歯医者サービス
* AI発注
    * 発注業務の自動化
    * <例>イトーヨーカドー堂、 2010年9月から食品を扱う全店舗でAI発注の仕組みを導入
        * 商品の販売実績、在庫の住居、天気などを踏まえて、最適な発注量を推計
            * 発注にかかる時間を約30%を削減
            * 欠品率を約120%削減する効果が得られた
* ピープルアナリティクス
    * 人材マネジメントに活用
        * 従業員の属性(性別、年齢、明既婚など)、行動、業務上の成果、評価等の情報をデータベース化して活用
        * 従来の人材マネジメントの場合は、社員のパフォーマンスは、配属部署任せになることが多かったが、人事部がデータ分析することで、全体最適となる人材マネジメントができるようになった
            * 活用できるデータ「 個人の属性データ、性格、思考データや、勤務や行動に関するデータ」
        * パフォーマンスを最大化する人材配置、従業員、満足の向上、ミスマッチを起こさない採用基準の設定等ができる

『データサイエンスの基礎知識』
【統計学の基礎】
* 分散(記号σ2 ← ２乗)
    * どのくらい散らばっているのかを1つの指標で表したもの
    * 各データと平均値の差分を2乗したものの平均
    * 各データは、平均的に、どれぐらい平均値から離れているのかを指評価したもの
    * データが尖ってた方がばらつき小さく、やわらかだと、なめらか
    * 欠点
        * 計算の過程で、データを2乗しているため、単位の意味が分かりにくいのが欠点
* 標本偏差(記号σ)
    * 分散の平方根(ルート)をとることで、２乗した単位をもとに戻したもの
    * <例>入試などで使われる偏差値は「10×(自分の点数 - 平均点) / 標準偏差+50」で計算される
* 中心極限定理
    * 母集団から抽出された標本で計算された平均値(標本平均)は、標本数が十分に多い場合に、正規分布の形になる。推定や検定などを行う際の基礎になる考え方。
* 信頼区間
    * 母集団の真の値(母平均など)が、標本データから計算された結果から、どのぐらいの区間に入るのかを推計した範囲のこと。
    * 【関連用語】
        * 信頼係数とは？
            * 信頼区間入る可能性
* 仮説検定
    * ある仮説のもと、標本データから得られた結果は、どれぐらいの確率で起こることなのかを計算し、仮説の正しさを判定するもの
    * 【関連用語】
        * 区間推定とは？
            * 標本平均から、母平均が、どのくらいの範囲に、何%の確率で収まるかと言う信頼区間を推計すること
            * 歩歩行で、どのくらいの範囲に入るのかと推計する
* 相関係数
    * 2つのデータの間にある関係の強さを指標化したもの
    * 各データの平均値との差を掛け合わせたものの平均である(共分散)と、各データの標準偏差から計算される
    * 【公式】
    * 共分散 Cov(X,Y) = (X-Xの平均) * (Y - Yの平均)
    * 相関係数 r(X, Y) =           XとYの共分散
							———————————————
							(Xの標準偏差) * (Yの標準偏差)
* ベイズ統計
    * ベイズの定理とは、特定の条件のもとで、ある事象が起こる確率(条件付き確率)をもとに、データを分析、解釈する理論
    * 見方を変えると言う点が「ベイズの定理」のポイントで、データを解釈する際の誤解を排除することができる
        * 味方を変えることで、結果に及ぼす要因を正しく評価できる
    * <例> 迷惑メールの判定があり、「メールの本文中に無料と言う表記があったら、迷惑メールと判断できるか」と言う問題。
        * 全体 100通
        * 迷惑メール　20通
            * うち無料という表記があったもの 6通(迷惑メールのなかで、無料表記は30%の割合)
        * 無料表記があるメール 10通 (全体の10%)
↓味方を変える
        * 無料表記があるメールのうち、迷惑メールだったもの　6通
            * →「無料」表記がある場合の迷惑メールの割合：60%
    * ベイジアンネットワーク
        * データの因果関係を分析する手法
        * 因果関係の強さを「条件付き確率」の考え方から判断し、多数の事象間の因果関係をグラフィカルに整理する
        * ベイズの定理を基本としているためベイジアンという名前がついている
        *         * 回帰分析とは違い、ある事象が起こった場合に、他の事象が起こる確率である「条件付き確率」から判断する
* モンティ・ホール問題
    * ベイズの定理を応用して考えられる代表的な事例
    * 3つの扉からプレゼントを当てるもので、回答者が一度、扉を選んだ後に、あたりを知っている司会者が、選ばれた扉以外の1つから、はずれの扉を教えてくれ、解答者は、最初に選んだ扉のままにすべきか、もう一つの扉に変えるべきかと言う問題。
    * 【結論】 変えた方が2倍正答率高くなる
        * 前提条件が大事 1回目は必ず外れの扉を教えてくれる
        * あたり　→　はずれ
        * はずれ　→　あたり
        * はずれ　→　あたり
        * という確率になるので、変えた方が2倍確率が高くなる
* 因果推論
    * 入力データと出力データからその因果関係を統計的に推定していく考え方
    * 原因となる要素に「接触した人(処置群)」と「接触しなかった人(対象群)」における結果を比較することで、因果関係を推定する
    * <例>広告効果の推計
        * もし広告に接触しなかったらどうなっていたのかを把握することで、因果関係を推定する
        * 広告の接触、非接触以外の条件を同一にして、処置郡と対象郡を比較する必要がある

【AI・機械学習・ディープラーニング、アルゴリズムまとめ】
* AI
    * AIの明確な定義はないが「人工的につくられる知能」
* 機械学習
    * 機会を用いて学習する方法
    * 『関連用語』
        * ニューラルネットワークとは？
            * 機会のルールの構造を検討する際に、脳の神経細胞のネットワークを元にした考え方
* ディープラーニング
    * ニューラルネットワークの中でも、構造をさらに多層化にすることで、より正しいルールを見つけようとする考え方
* アルゴリズムまとめ
    * 【教師あり学習】
        * 入力データと出力データ(答え)が揃っている
        * 【回帰分析】
            * 説明変数と目的変数の関係性を比べて、関係性式で表す分析手法
            * 説明変数がひとつの場合は「単回帰」
                * <例>身長から体重を推計する場合
            * 説明変数が複数ある場合は「重回帰」と呼ぶ
                * <例>身長・体脂肪・年齢などの複数の変数から体重を推計する
                * 多重共線性
                    * 重回帰分析において、2つ以上の説明変数が高い線形関係にある状況のこと
            * <ユースケース>販売料予測、機械の異常検知
            * 【回帰分析におけるP値】
                * 回帰分析において計算された係数が、統計的に正しいといえるかどうかを検討するための指標
                * P値のPは確率を表すProbabilityのP
            * 【代表例】
                * 線形回帰
                    * よく見る斜めの相関しているっていうグラフのやつ
                * 時系列分析
                    * 時系列分析は、時間の経過順に並んだデータをもとに、変動要因を長期的な変動、周期的な変動、不規則な変動等の要素に分解し、将来の話題を予測するアルゴリズム
                    * 🅿️過去のデータの波形から、3つの変動要因(長期、季節、不規則)でデータを分解できるかどうか
        * 【分類】
            * 与えられたデータに適切なクラスを割り当てる
            * <ユースケース>迷惑メール判定、手書き文字認識、カード不正検知
            * 【代表例】
                * ロジスティック回帰
                    * 目的変数が2値(0と1)の場合に、説明変数との関係を指揮で表す分析方法
                    * 各説明変数の時の確率を推計し、確率が0.5を超えるかどうかで2値の判定を行う
                * 決定木
                    * 目的変数の予測モデルで、影響を及ぼしている説明変数をツリー状にして整理するアルゴリズム
                    * <例>アイスの売上に影響する曜日、気温、天気を明らかにして、説明変数の構造をツリー状のモデルとして作成し分析する
                    * LightGBM
                        * 決定木の一種
                        * 複数回の決定木分析を行う
                        * 一つ一つの決定木の精度をなるべく落とさずに、高速に構築できるようになるので、誰もが利用できるという特徴もある
    * 【教師なし学習】難易度
        * 一連の入力データから背景に隠れたパターンを見つける
        * クラスタリング
            * 値の類似性をもとにデータをグループ化
            * <ユースケース>顧客のセグメント化
            * 【代表例】k-means
                * データをクラスタリングする代表的なアルゴリズム
                * 各クラスタの重心と各データの距離を計算して、新しいクラスタを設定することを繰り返し実施する
                * 簡単に実装でき、計算も速いため、誰もが使うことができるのが最大の特徴
        * 次元削減・情報圧縮
            * データの特徴を残しながら簡素化
            * <ユースケース>顔認証、商品類似性判断
            * 【代表例】主成分分析
                * たくさんの変数から、新たな変数を作成することで変数を削減するアルゴリズム
                * 元のデータの特徴をできるだけ保つ形で変数の数を削減する
                * K-meansが縦方向の要約だとしたら、主成分分析は横方向の要約
                * <例>学校のテスト科目で、国語・社会・英語を「文系」能力、数学・理科を「理系」能力というようにまとめる考え方
    * Prophet
        * 日次データの構造を分割して、将来予測を行うアルゴリズム
        * Metaが2017年に開発したもの

🔵分析の問題と解決方法のため発明された技術🔵
* 過学習
    * 機械学習などにおいて、規則性を見つける作業した際に、与えられたデータだけに適応した学習を過剰にしてしまい、道のデータにおける予測がうまくいかない状態
    * 過学習を避けるための方法
        * 正則化
            * 説明する側の式の複雑さが増すことに罰則をつけて学習させる
        * ホールドアウト検証
            * 元データを学習用のデータと、検証用のテストデータに分けて学習結果を評価する方法
        * クロスバリデーション
            * 分析の元となるデータを複数個に分割して、様々なデータの組み合わせで学習・検証する方法
            * 🔵メリット🔵
                * 分割による偏りから生じる過学習のリスクを軽減できる
            * 与えられたデータでの精度が高くても、予測の精度は必ずしも高くならない可能性がある
                * <例>偶然、どちらのデータでも精度の高いモデルができてしまう場合
* AutoML(自然機械学習)❓Automated Machine Learningの略
    * 機械学習を用いた分析で様々なタスクを自動化する技術のこと
    * アルゴリズムの無いようは高度化、専門家しているため、ある程度コンピューターで機械的に判断できないだろうかというニーズが高まったため開発された
    * 複数のハイパーパラメータと呼ばれる値を設定する必要があり、この設定値によって結果の精度などが変化する
    * 精度の高いあるごリムの選択とハイパーパラメータの調整が主たる役割
    * <例>A・B・C・Dと分割して、４つのモデルの精度の平均をとって、モデルを評価する
    * K-分割交差検証とも呼ばれる
        * K個のデータセットに分割するため
* ブラックボックス問題
    * 精度は高いが、なぜ、そういう予測結果になったのかが分からないという問題
    * ディープラーニングのモデルは非常に多層化・複雑化していて、個々に内部構造を理解するのは不可能
        * <例>猫の画像かどうかを判断する例、どう判断しているか分からない
        * 最近は説明可能なAI、ホワイトボックス型のAIが注目されている
            * 精度と納得生の両方を持った機械学習のアルゴリズムが求められている

『50代がなぜデータサイエンティストとして強いのか』
* 2021年データサイエンスの割合
    * 統計や、機械学習などの知識と、それらを活用できるスキルが求められるが、活用する先の業界や業務のことを知らなければ、本来の力を発揮することが難しいが、50代は、データサイエンティストに、求められる特有の業界などの知識を持っているから多い→豊富な知識と経験から、上手に短縮することはできるかもしれない？
    * 20代9%
    * 30代24%
    * 40代32%
    * 50代26%
    * 60代9% 
